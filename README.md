- Github: [stefann](https://github.com/prasunroy/stefann)

# Paper Summary
- Paper: [STEFANN: Scene Text Editor using Font Adaptive Neural Network](https://openaccess.thecvf.com/content_CVPR_2020/papers/Roy_STEFANN_Scene_Text_Editor_Using_Font_Adaptive_Neural_Network_CVPR_2020_paper.pdf)
## Methodology
- In this paper, we propose a method to modify text in an image at character-level. We approach the problem in two stages. At first, the unobserved character (target) is generated from an observed character (source) being modified.
- We propose two different neural network architectures – (a) FANnet to achieve structural consistency with source font and (b) Colornet to preserve source color. Next, we replace the source character with the generated character maintaining both geometric and visual consis- tency with neighboring characters.
- However, if any alteration is required in the text present in an image, it becomes extremely difficult for sev- eral reasons. *For instance, a limited number of observed characters makes it difficult to generate unobserved charac- ters with sufficient visual consistency. Also, different natural conditions, like brightness, contrast, shadow, perspective distortion, complex background, etc., make it harder to replace a character directly in an image.* The main motivation of this work is to design an algorithm for editing textual information present in images in a convenient way similar to the conventional text editors.
- To the best of our knowledge, this is the first work that attempts to modify texts in scene images. *For this purpose, we design a generative network that adapts to the font features of a single character and generates other necessary characters. We also propose a model to transfer the color of the source character to the target character.*
- **The entire process works without any explicit character recognition. To restrict the complexity of our problem, we limit our discussion to the scene texts with upper-case non-overlapping characters. However, we demonstrate in Figs. 5 and 13 that the proposed method can also be applied for lower-case characters and numerals.**
- The proposed method is composed of the following steps: (1) Selection of the source character to be replaced, (2) Generation of the binary target character, (3) Color transfer and (4) Character placement. *In the first step, we manually select the text area that requires to be modified. Then, the algorithm detects the bounding boxes of each character in the selected text region. Next, we manually select the bounding box around the character to be modified and also specify the target character. Based on these user inputs, the target character is generated, colorized and placed in the inpainted region of the source character.* (Comment: 교체할 문자를 지정하는 데 있어서 일부 수작업이 들어갑니다.)
## Related Works
- Though GAN-based font synthesis could be used to estimate the target character, several challenges make the direct implementation of font synthesis for scene images difficult.
- Secondly, it is often observed that a particular word in an image may have a mixture of different font types, sizes, colors, etc. Even depending on the relative location of the camera and the texts in the scene, each character may experience a different amount of perspective distortion. Some GAN-based models require multiple observations of a font-type to faithfully generate unobserved characters. A multiple observation-based generation strategy requires a rigorous distortion removal step before applying generative algorithms. Thus, rather than a word-level generation, we follow a character-level generative model to accommodate maximum flexibility.
### Text Recognition
- **Firstly, most of the GAN-based font synthesis models require an explicit recognition of the source character. As recognition of text in scene images is itself a challenging problem, it is preferable if the target characters can be generated without a recognition step. Otherwise, any error in the recognition process would accumulate, and make the entire text editing process unstable.**
- *Moreover, most of the algorithms require explicit recognition of the source characters to generate the unseen character set. This may create difficulty in our problem as text recognition in scene images is itself a challeng- ing problem and any error in the recognition step may affect the entire generative process.*
- *Character generation from multiple observations is also challenging for scene images as the observed characters may have distinctively different characteristics like font types, sizes, colors, perspective distortions, etc.*
## FANnet
- Our generative font adaptive neural network (FANnet) takes two different inputs – an image of the source character of size 64 × 64 and a one-hot encoding 'v' of length 26 of the target character. For example, if our target character is 'H', then 'v' has the value 1 at index 7 and 0 in every other location.
- In Fig. 6, we show the average SSIM of the generated characters for each different source character. *It can be seen from the ASSIM scores that some characters, like 'I' and 'L', are less informative as they produce lower ASSIM values, whereas characters, like 'B' and 'M', are structurally more informative in the generation process.*
## Colornet
- For a natural-looking generation, it is also important to transfer the color and texture of the source character to the generated character. *In this work, we introduce a CNN-based color transfer model that takes the color in- formation present in the source character and transfer it to the generated target character. The proposed color transfer model not only transfers solid colors from source to target character, it can also transfer gradient colors keeping subtle visual consistency.*
- *We propose a CNN based architecture, named Colornet, that takes two images as input – colored source character image and binary target character image. It generates the target character image with transferred color from the source character image.*
- We train Colornet with synthetically generated image pairs. *For each image pair, the color source image and the binary target image both are generated using the same font type randomly selected from 1300 fonts. The source color images contain both solid and gradient colors so that the network can learn to transfer a wide range of color variations. We perform a bitwise-AND between the output of Colornet and the binary target image to get the final col- orized target character image.*